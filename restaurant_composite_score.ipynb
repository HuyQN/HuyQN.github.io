{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dea7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/fahimkhan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/fahimkhan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fahimkhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fahimkhan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Fahim Khan - fkhan99@gatech.edu\n",
    "# DVA Spring 2025 - Final Project\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.util import *\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet') \n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 200)\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f69fb9",
   "metadata": {},
   "source": [
    "Review Dataset Columns\n",
    "0\trecord\n",
    "1\treview_id\n",
    "2\tuser_id\n",
    "3\tbusiness_id\n",
    "4\tstars\n",
    "5\tuseful\n",
    "6\tfunny\n",
    "7\tcool\n",
    "8\ttext\n",
    "9\tdate\n",
    "10\tname\n",
    "11\taddress\n",
    "12\tcity\n",
    "13\tstate\n",
    "14\tpostal_code\n",
    "15\tbusiness_stars\n",
    "16\treview_count\n",
    "17\tis_open\n",
    "18\tattributes\n",
    "19\tcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "2df0169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_id', 'user_id', 'business_id', 'stars', 'text', 'date', 'name', 'city']\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "cols = [1,2,3,4,8,9,10,12,19]\n",
    "reviews = pd.read_csv('fused_reviews_202503152331.csv',  usecols=cols) # nrows=100)\n",
    "reviews = reviews[reviews['categories'].str.contains('Restaurants', na=False)]\n",
    "allowed_cities = ['Philadelphia', 'New Orleans', 'Tampa', 'Tucson', 'Nashville', 'Indianapolis']\n",
    "reviews = reviews[reviews['city'].isin(allowed_cities)]\n",
    "reviews = reviews.drop(['categories'], axis=1)\n",
    "\n",
    "print(reviews.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "98aada8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002068358461654562 min / max 1.0\n",
      "['review_id', 'user_id', 'business_id', 'stars', 'text', 'name', 'city', 'recency_score']\n"
     ]
    }
   ],
   "source": [
    "# Add recency score\n",
    "reviews['date'] = pd.to_datetime(reviews['date'])\n",
    "today = pd.Timestamp(datetime.today().date())\n",
    "reviews['age'] = (today - reviews['date']).dt.days \n",
    "reviews['age'] = reviews['age'] - reviews['age'].min()\n",
    "lambda_val = 0.001 # half life of two years\n",
    "reviews['recency_score'] = np.exp(-lambda_val * reviews['age'])\n",
    "print(reviews['recency_score'].min(),\"min / max\", reviews['recency_score'].max())\n",
    "reviews = reviews.drop(['age','date'], axis=1)\n",
    "\n",
    "print(reviews.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "57372657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_id', 'business_id', 'stars', 'text', 'name', 'city', 'recency_score', 'credibility', 'credibility_norm']\n"
     ]
    }
   ],
   "source": [
    "# Join with User Credibility data\n",
    "user_cred = pd.read_csv('user_postprocess.csv')\n",
    "reviews = pd.merge(reviews, user_cred, how='left', on='user_id')\n",
    "# Normalize credibility\n",
    "cred_min = reviews['credibility'].min()\n",
    "cred_max = reviews['credibility'].max()\n",
    "reviews['credibility_norm'] = (reviews['credibility'] - cred_min) / (cred_max - cred_min + 1e-9)\n",
    "reviews = reviews.drop(['Unnamed: 0','user_id'], axis=1)\n",
    "\n",
    "print(reviews.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "a9f02592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review_id', 'business_id', 'stars', 'text', 'name', 'city', 'recency_score', 'credibility', 'credibility_norm', 'sentiment', 'normalized_sentiment']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "reviews['tokens'] = reviews['text'].str.lower().apply(word_tokenize)\n",
    "# Remove punctuation\n",
    "reviews['tokens'] = reviews['tokens'].apply(lambda tokens: [word for word in tokens if word not in string.punctuation])\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "reviews['tokens'] = reviews['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "reviews['tokens'] = reviews['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
    "# Join back into string\n",
    "reviews['joined_text'] =  reviews['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "# Polarity scores\n",
    "reviews['sentiment'] = reviews['joined_text'].apply(lambda text: sia.polarity_scores(text)['compound'])\n",
    "# Normalize\n",
    "min = reviews['sentiment'].min()\n",
    "max = reviews['sentiment'].max()\n",
    "reviews['normalized_sentiment'] = (reviews['sentiment'] - min) / (max - min)\n",
    "reviews = reviews.drop(['tokens', 'joined_text'], axis=1)\n",
    "\n",
    "print(reviews.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "4b08047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business_id', 'name', 'city', 'final_score', 'stars', 'normalized_sentiment', 'credibility_norm', 'recency_score']\n"
     ]
    }
   ],
   "source": [
    "# Compute final weighted score\n",
    "reviews['final_score'] = reviews['normalized_sentiment'] * (0.5 + 0.25 * reviews['recency_score'] + 0.25 * reviews['credibility_norm'])\n",
    "\n",
    "# Group and sort\n",
    "rest_level_sentiment = reviews.groupby(['business_id','name','city'], as_index=False)[['final_score', 'stars','normalized_sentiment', 'credibility_norm','recency_score']].mean()\n",
    "sorted_grouped = rest_level_sentiment.sort_values(['final_score'], ascending=False)   \n",
    "\n",
    "print(sorted_grouped.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9bc20cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "sorted_grouped.to_csv('restaurant_level_final_score.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
